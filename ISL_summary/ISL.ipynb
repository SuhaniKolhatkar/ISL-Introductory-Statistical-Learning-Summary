{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9468c5-6c53-407b-a62a-07dec834d2d8",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e5834-f871-4d0d-bbd6-2abed44c4e9d",
   "metadata": {},
   "source": [
    "# 1.Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499aebe5-f10d-4570-a94e-e5df0fd38830",
   "metadata": {},
   "source": [
    "# 2. Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ffa0d-b1c8-4f53-902b-815e9b83e4c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:red\"> 2.1 What is Statistical Learning ?</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5184ab-9b42-4840-9774-23b3c60790de",
   "metadata": {},
   "source": [
    "- <b>$X1$</b>, <b>$X2$</b>, <b>$X3$</b>, ... , <b>$Xp$</b> : Input variable,predicators, independent variables, features, variables\n",
    "  <b>$Y$</b> : Dependent variable \n",
    "- Relationship between Y and X = (<b>$X1$</b>, <b>$X2$</b>, <b>$X3$</b>, ... , <b>$Xp$</b>) can be written in general form :\n",
    "  <b>$Y$</b> = $f$(<b>$X$</b>) + $\\epsilon$\n",
    "    - $\\epsilon$ : random error term which is independent of $X$ and has $mean$ zero\n",
    "    - $f$ is the binding between the dependent and independent variables but that fuction is unknown heence there is a need to $estimate f $ based   on the observed points.\n",
    "- Statistical learning : Set of approaches for estimating $f$\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab4cfc7-b6e0-4d14-9b57-7622e2f87e12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> 2.1.1 Why estimate $f$</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb60149-cc01-4a47-a376-2aecd4d6a4db",
   "metadata": {},
   "source": [
    "Reason to estimate $f$\n",
    "1. Prediction \n",
    "2. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86d950-7a55-45fb-81c4-d00a3e12ab2f",
   "metadata": {},
   "source": [
    "#### 1. Prediction\n",
    "\n",
    "- In many situations, a set of inputs $X$ are readily available, but the output\n",
    "$Y$ cannot be easily obtained. In this setting, since the error term averages\n",
    "to zero, we can predict $Y$ using : <br>\n",
    "   \n",
    "  $\\hat Y$ = $\\hat f$($X$)<br>\n",
    "  $\\hat Y$ : resulting prediction/estimation for $Y$ <br>\n",
    "  $\\hat f$ : resulting prediction/estimation for $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbbc1f-07b3-4377-b6b9-3e5dc37b54ab",
   "metadata": {},
   "source": [
    "- In this setting, $\\hat f $ is <b>often treated as a black box</b>, in the sense\n",
    "that one is not typically concerned with the exact form of $\\hat f $, provided that\n",
    "it yields accurate predictions for $Y$ .\n",
    "- The accuracy of   $\\hat Y$ as a prediction for $Y$ depends on two quantities,which we will call the reducible error and the irreducible error. \n",
    "- $f$  will not be a perfect estimate for f, and this inaccuracy will introduce some error.\n",
    "- reducible error : we can potentially improve the accuracy of $\\hat f$($X$) by using the most appropriate statistical learning technique to estimate $f$.\n",
    "- we have <b>$Y$</b> = $f$(<b>$X$</b>) + $\\epsilon$\n",
    "- irreducible error : This is because $Y$ is also a function of epsilon, which, by defnition, cannot be predicted using $X$. Therefore, variability associated with \" also afects the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef685d3-7034-4654-9551-97a8d0baf746",
   "metadata": {},
   "source": [
    "<img src=\"img_1.JPG\" alt=\"image\" width=\"500\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef74e57-96b6-4650-bbb6-ca6ecd1fd134",
   "metadata": {},
   "source": [
    "- $E$($Y$ - $\\hat Y$)$^2$ : represents the expected value (mean) of the squared difference between the actual $Y$ and predicted $\\hat Y$.\n",
    "- Var($\\epsilon$) : denotes variance associated with the error term "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303dbd5-308d-4250-82e2-eb4c5fb4c8e5",
   "metadata": {},
   "source": [
    "NOTE : irreducible error will always provide an upper bound on the accuracy of our prediction for $Y$ . This bound is almost always unknown in practice.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41794e4-8eec-4e70-b2e8-848e820049e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Inference \n",
    "\n",
    "- We are often interested in understanding the association between $Y$ and\n",
    "$X1,...,Xp$. \n",
    "- In this situation we wish to estimate $f$, but our goal is not\n",
    "necessarily to make $predictions$ for $Y$ . Now <b> $\\hat f$  cannot be treated as a black\n",
    "box, because we need to know its exact form.</b>\n",
    "\n",
    "\n",
    "<img src=\"img_2.JPG\" alt=\"image\" width=\"600\" height=\"auto\">\n",
    "\n",
    "\n",
    "Img description : The Advertising data set. The plot displays sales, in thousands\n",
    "of units, as a function of TV, radio, and newspaper budgets, in thousands of\n",
    "dollars, for 200 diferent markets. In each plot we show the simple least squares\n",
    "ft of sales to that variable, as described in Chapter 3. In other words, each blue\n",
    "line represents a simple model that can be used to predict sales using TV, radio,\n",
    "and newspaper, respectively\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50110c9-f25b-4a91-84d4-bef507ee5ea0",
   "metadata": {},
   "source": [
    "##### Example including both $prediction$ and $inference$\n",
    "\n",
    "real estate setting, one may seek to relate values of homes to inputs :\n",
    "- crime rate \n",
    "- zoning\n",
    "- distance from a river \n",
    "- air quality\n",
    "- schools \n",
    "- income level of community\n",
    "- size of houses\n",
    " </br>\n",
    " \n",
    " <u>$inference$</u> : <span style=\"color:green\">association between each <i>individual input variable</i> and <i>housing price </i> </span> for instance, <b>how much extra will a house be worth if it has a view of the river?</b> </br>\n",
    "<u>$prediction$</u> : <span style=\"color:green\">predicting the value of a\n",
    "home given its characteristics:  </span><b>is this house under- or over-valued?</b>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af80336-0133-4994-aff0-64bb79dd40f8",
   "metadata": {},
   "source": [
    "> NOTE :  In contrast, some of the highly non-linear approaches\n",
    "that we discuss in the later chapters of this book can potentially provide\n",
    "quite accurate predictions for Y , but this comes at the expense of a less\n",
    "interpretable model for which inference is more challenging\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff50d3-934a-4a6e-af4d-4a9f4bdb6382",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> 2.1.2 How do we estimate $f$ ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688a95e-2aa7-43f3-a205-ca00044873b2",
   "metadata": {},
   "source": [
    "- Assumption : We will always assume that we have observed a set of n diferent data points\n",
    "- training data because we will use these observations to train, or teach, our method how to estimate $f$. \n",
    "\n",
    "- $xij$ : value of $j$ th predictor for $i$ th observation \n",
    "\n",
    "<img src=\"img_3.JPG\" alt=\"image\" width=\"700\" height=\"auto\">\n",
    "\n",
    "- $y_i$ : response variable for the $i$th observation.\n",
    "- Then our training data consist of :\n",
    "\n",
    "<img src=\"img_5.JPG\" alt=\"image\" width=\"600\" height=\"auto\">\n",
    "<img src=\"img_4.JPG\" alt=\"image\" width=\"500\" height=\"auto\">\n",
    "<br>\n",
    "<br>\n",
    "Our goal is to apply a statistical learning method to the training data\n",
    "in order to estimate the unknown function $f$. In other words, we want to\n",
    "fnd a function $\\hat f $ such that Y ≈ $\\hat f $ for any observation ($X$ , $Y$). <br>Broadly\n",
    "speaking, most statistical learning methods for this task can be characterized as either <b>$parametric$</b> or <b>$non-parametric$</b> \n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02893258-4c0a-4759-a22c-e18a7d34671f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### <span style=\"color:blue\"> Parametric Method</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed952650-b48d-4de3-8dec-ef4f35ae62fb",
   "metadata": {},
   "source": [
    "- two-step model-based approach.\n",
    "1. First, we make an assumption about the functional form, or shape, of $f$. For example, one very simple assumption is that <b> f is linear in </b> X \n",
    "\n",
    "   #### $f$($X$) ≈ $\\beta_0$ + $\\beta_1$ $X_1$ + $\\beta_2$ $X_2$ + ··· + $\\beta_p$ $X_p$.\n",
    "\n",
    "    - Instead of having to estimate an entirely arbitrary $p$ -dimensional function $f$($X$), one only needs to estimate the $p + 1$ coefcients $\\beta_0$, $\\beta_1$, $\\beta_2$, . . . , $\\beta_p$\n",
    "2. After a $linear \\; model$ has been selected, we need a procedure that uses the training data to fit or train the model. In the case of the linear model fit, we need to estimate the parameters $\\beta_0$, $\\beta_1$, $\\beta_2$, . . . , $\\beta_p$. That is, we want to find values of these parameters such that : <br>\n",
    "    #### $Y$ ≈ $\\beta_0$ + $\\beta_1$ $X_1$ + $\\beta_2$ $X_2$ + ··· + $\\beta_p$ $X_p$.\n",
    "    \n",
    "___\n",
    "\n",
    "<img src=\"img_7.JPG\" alt=\"image\" width=\"500\" height=\"auto\"> <img src=\"img_8.JPG\" alt=\"image\" width=\"500\" height=\"auto\">\n",
    "\n",
    "___\n",
    "\n",
    "- Most common approach to fit linear model is method of least square methods but there are other approaches to fit the linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c06e8-3ace-49a2-ab32-9d5f5a6405a1",
   "metadata": {},
   "source": [
    "> NOTE:\n",
    "1. The model-based approach is referred to as parametric; it reduces the problem of estimating $f$ down to one of estimating a set of $\\beta_0$, $\\beta_1$, $\\beta_2$, . . . , $\\beta_p$ parameters.Since we assume $f$ a parametric form of above mentioned parameters in linear model.\n",
    "2. disadvantage of a parametric approach is that the model we choose will usually not match the trueunknown form of f.\n",
    "3. We can try to address this problem by choosing fexible models that can fit many diferent possible functional forms fexible for f. But in general, ftting a more fexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as $overftting$ $the$ $data$, which essentially means they $overftting$ follow the errors, or noise, too closely.\n",
    "\n",
    "___\n",
    "\n",
    "<img src=\"img_8.JPG\" alt=\"image\" width=\"600\" height=\"auto\">\n",
    "Fitted linear model : <img src=\"img_9.JPG\" alt=\"image\" width=\"500\" height=\"auto\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1f0b14-a1d5-42f9-bdd4-4ec9f798cb88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### <span style=\"color:blue\"> Non-Parametric Method</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ff6d3-33db-468f-b234-35056bebc909",
   "metadata": {},
   "source": [
    "- does not assume any form of $f$\n",
    "- they seek an estimate of f that gets as close to the data points as possible.\n",
    "-  by <b>avoiding the assumption of a particular functional form for </b> $f$ , they have the potential to accurately fit a wider range of possible shapes for $f$.\n",
    "- Disadvantage : since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$.\n",
    "\n",
    "---\n",
    "\n",
    "A thin-plate spline is used to estimate f. This approach does not impose any pre-specifed model on $f$ . It instead attempts to produce an estimate for f that is as close as possible to the observed data, subject to the ft—that is, the yellow surface as shown in the figure below : \n",
    "<br>\n",
    "<br>\n",
    "<img src=\"img_10.JPG\" alt=\"image\" width=\"600\" height=\"auto\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd4da59-30f0-4503-96aa-77151ffc1cd2",
   "metadata": {},
   "source": [
    "##### Example of <b>overftting</b> : \n",
    "Undesirable situation because the ft obtained will not yield accurate estimates of the response on new observations that were not part of the original training data set.\n",
    "<br>\n",
    "<img src=\"img_11.JPG\" alt=\"image\" width=\"600\" height=\"auto\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeaaba9-f4f5-4edf-b0d1-aa86a905f94c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> 2.1.3 The trade-off between Prediction Accuracy and Model Interpritability  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c1afe-dbcd-434a-8625-17a6af3b598e",
   "metadata": {},
   "source": [
    "A representation of the tradeof between fexibility and interpretability, using diferent statistical learning methods. <br> In general, <b>as the fexibility of a method increases, its interpretability decreases.</b>\n",
    "\n",
    "<img src=\"img_12.JPG\" alt=\"image\" width=\"800\" height=\"auto\">\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf2ea8-a45f-4015-a18b-f7a301740aa2",
   "metadata": {},
   "source": [
    " Why would we ever choose to use a more restrictive method instead of a very fexible approach?\n",
    "- we are mainly interested in inference, then restrictive models are much more interpretable.\n",
    "    -  when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between $Y$ and $X_1$, $X_2$ ,...,$X_p$.\n",
    "\n",
    "Sometimes if the Prediction is the goal then using Flexible model is of the use. (Sometimes less flexible model can alos achieve accurate prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d26b25-1ce4-4bff-a043-fd0f11333297",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> 2.1.4 Supervised Versus Un-Supervised learning </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c8395-9935-41e2-95eb-c442dfd21c5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "- Most statistical learning problems fall into one of two categories: supervised and unsupervised.\n",
    "\n",
    "---\n",
    "\n",
    "#### Supervised learning :\n",
    "-  we have $x_i$ ,  $i$ = 1, 2, ... , $n$ observations for the predictor(s).\n",
    "- there is an associated response $y_i$.\n",
    "- here our aim is :\n",
    "    -  to ft a model that relates the response to the predictors\n",
    "    - Prediction \n",
    "        - accurately predicting the response for future observations \n",
    "    - Inference\n",
    "        - better understanding the relationship between the response and the predictors\n",
    "        \n",
    "---\n",
    "\n",
    "#### Un-Supervised learning :\n",
    "- situation in which for every observation i = 1,...,n, we observe a vector of measurements $x_i$ but no associated response $y_i$. \n",
    "- It is not possible to ft a linear regression model, since there is no response variable to predict.\n",
    "- we lack a response variable that can supervise our analysis.\n",
    "- to understand the relationships between the variables or between the observations.\n",
    "- One of the examples for doing un- supervised learning :\n",
    "    - Cluster analysis or Clustering :\n",
    "        - . The goal of cluster analysis cluster analysis is to ascertain, on the basis of $x_i$,...,$x_n$ whether the observations fall into relatively distinct groups\n",
    "        - Example: \n",
    "            - market segmentation study we might observe multiple characteristics (variables) for potential customers,such as zip code, family income, and shopping habits\n",
    "            - We might believe that the customers fall into diferent groups, such as big spenders versus low spenders.\n",
    "            - . If the information about each customer’s spending patterns were available, then a supervised analysis would be possible.However, this information is not available—that is, we do not know whether each potential customer is a big spender or not.\n",
    "            - In this setting, we can try to cluster the customers on the basis of the variables measured, in order to identify distinct groups of potential customers.\n",
    "            - Identifying such groups can be of interest because it might be that the groups difer with respect to some property of interest, such as spending habits.\n",
    "            - <i> A clustering data set involving three groups. Each group is shown using a diferent colored symbol. Left: The three groups are well-separated. In this setting, a clustering approach should successfully identify the three groups. Right: There is some overlap among the groups. Now the clustering task is more challenging<br>\n",
    "</i> <img src=\"img_13.JPG\" alt=\"image\" width=\"600\" height=\"auto\">\n",
    "                - goal is to determine the group to which each observation belongs.\n",
    "                - We have plotted 150 observations with measurements on two variables, X1 and X2. Each observation corresponds to one of three distinct groups. For illustrative purposes, we have plotted the members of each group using diferent colors and symbols. However, in practice the <b>group memberships are unknown, and the goal is to determine the group to which each observation belongs.</b>\n",
    "                - In the left-hand panel of the above fig, this is a relatively easy task because the groups are well-separated. By contrast, the right-hand panel illustrates a more challenging setting in which there is some overlap between the groups. A clustering method could not be expected to assign all of the overlapping points to their correct group (blue, green, or orange).\n",
    "                - For instance, if there are p variables in our data set, then $p(p − 1)/2$ distinct scatterplots can be made, and visual inspection is simply not a viable way to identify clusters. For this reason, automated clustering methods are important.\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6e4f3-d3c0-4787-aabc-83867fe6afe8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> 2.1.5 Regression Versus Classifcation Problems </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b09165-9f4a-47c0-b008-b50c21acd84d",
   "metadata": {},
   "source": [
    "- Variables \n",
    "    - Qualitative :\n",
    "        - Categorical \n",
    "    -  Quantitative :\n",
    "        - Numerical \n",
    "- Quantitative response : Linear Regression problems \n",
    "- Qualitative response : Classification problems (Logistic Regression*)\n",
    "- We tend to select statistical learning methods on the basis of whether the <b>response</b> is quantitative or qualitative; i.e. we might use linear regression when quantitative and logistic regression when qualitative.\n",
    "- whether the <b>predictors</b> are qualitative or quantitative is generally considered less important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efde2b8-096a-4daa-9d21-fe77b78b6430",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:red\"> 2.2 Assessing Model Accuracy</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65d324-1c5c-48bc-b0be-6add63b6bfb1",
   "metadata": {},
   "source": [
    "- it is an important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.\n",
    "-  we discuss some of the most important concepts that arise in selecting a statistical learning procedure for a specifc data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98487d-acb3-4227-a537-aeeecec0965a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> 2.2.1 Measuring the Quality of Fit</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0237cf-2c04-44ca-8304-dabe0e7a8453",
   "metadata": {},
   "source": [
    "- Evaluating performance : How well the predicted results match with the actual results  : Least error \n",
    "- quantification of the extent to which the $predicted$ $response$ $value$ for a given observation is close to the $true$ $response$ $value$ for that observation.\n",
    "- Regression setting :\n",
    "    - Commmonly used setting : $Mean$ $Squared$ $Error$\n",
    "        <br> <img src=\"img_14.JPG\" alt=\"image\" width=\"250\" height=\"auto\">\n",
    "        - $\\hat f(x_i) $ is the prediction that $\\hat f $ gives for the ith observation\n",
    "        - $MSE$ >> if actual response differs with the predicted response for some or many observations \n",
    "        - $MSE$ << if actual response does not much differ with the predicted response for given observations \n",
    "    - $Traning$ $MSE$ \n",
    "        - aboove mentioned MSEcomputed using the training data that was used to fit the model, and so should more accurately be referred to as the$Traning$ $MSE$.\n",
    "- we do not really care how well the method works on the training data. Rather, we are interested in the <b>accuracy of the predictions that we obtain when we apply our method to previously unseen test data</b>\n",
    "- Mathematical explanation :\n",
    "    - Total observations = Training obs + Testing obs \n",
    "    - we fit our statistical learning method on our training observations {($x_1$, $y_1$),($x_2$, $y_2$),...,($x_n$, $y_n$)}, and we obtain the estimate $\\hat f $.We can compute $\\hat f(x_1) $, $\\hat f(x_2) $, ... , $\\hat f(x_n) $\n",
    "    - If these are approximately equal to $y_1$, $y_2$,...,$y_n$ then the training MSE given by above equation is small.\n",
    "        - However, we are really not interested in whether $\\hat f(x_i)$ ≈ $y_i$\n",
    "        - we want to know whether $\\hat f(x_0) $ is approximately equal to $y_0$, where ($x_0$,$y_0$) is a previously unseen test observation not used to train the statistical learning method. \n",
    "        - We want to choose the method that gives the <b>lowest test MSE</b>, as opposed to the <b>lowest training MSE</b>.\n",
    "        - If we have large testing observations then we can compute : <br><img src=\"img_15.JPG\" alt=\"image\" width=\"150\" height=\"auto\"><br>we are interested in method which gives us the above value lowest\n",
    "        <br>\n",
    "- If we have test data set : \n",
    "    - We can minimise the test MSE and check the approriate statistical learning method \n",
    "- If we dont have test data set : \n",
    "    -  simply selecting a statistical learning method that minimizes the training MSE. \n",
    "    -  fundamental problem with this strategy:\n",
    "        - there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE.\n",
    "        - many statistical methods specifcally estimate coefcients so as to minimize the training set MSE.\n",
    "        - <br> <img src=\"img_16.JPG\" alt=\"image\" width=\"700\" height=\"auto\">\n",
    "            - Left hand panel \n",
    "                - Black curve : true $f$\n",
    "                - The orange, blue and green curves illustrate three possible estimates for f obtained using methods with increasing levels of fexibility.\n",
    "                    - Orange line : Linear Regression fit \n",
    "                    - Blue and green curves : smoothning splines \n",
    "                - It is clear that as the level of fexibility increases, the curves fit the observed data more closely.\n",
    "            - Right hand panel \n",
    "                - Grey  curve : the average training MSE as a function of fexibility, or more formally the degrees of freedom, for a number of smoothing splines.\n",
    "                    - The degrees of freedom is a quantity that summarizes the fexibility of a curve. \n",
    "                - The orange, blue and green squares indicate the MSEs associated with the corresponding curves in the lefthand panel.\n",
    "                - The training MSE declines monotonically as fexibility increases.\n",
    "                - true $f$ is non-linear, and so the orange linear fit is not fexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it corresponds to the most fexible of the three curves fit in the left-hand panel.\n",
    "                - Red  curve : the average test MSE as a function of fexibility\n",
    "                - test MSE initially declines as the level of fexibility increases. However, at some point the test MSE levels of and then starts to increase again\n",
    "                - the orange and green curves both have high test MSE. The blue curve minimizes the test MSE.\n",
    "                - The horizontal dashed line indicates Var($\\epsilon$), the irreducible error, which corresponds to the lowest achievable test MSE among all possible methods. Hence, the smoothing spline represented by the blue curve is close to optimal.\n",
    "                \n",
    "---\n",
    "\n",
    "NOTE : \n",
    "1.  As the fexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used.\n",
    "2.  As model fexibility increases, the training MSE will decrease, but the test MSE may not.\n",
    "3.  When a given method yields a small training MSE but a large test MSE, we are said to be overftting the data. This happens because our statistical learning procedure is working too hard to fnd patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function $f$. \n",
    "4. regardless of whether or not overftting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE.\n",
    "5.  Overftting refers specifcally to the case in which a less fexible model would have yielded a smaller test MSE.\n",
    "\n",
    "---\n",
    "\n",
    "Few examples :\n",
    "<br> <img src=\"img_17.JPG\" alt=\"image\" width=\"700\" height=\"auto\"> <img src=\"img_18.JPG\" alt=\"image\" width=\"700\" height=\"auto\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce2e25-ac2d-4c4a-93a4-9c3caf74a80e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> 2.2.2 The Bias-Variance Trade-Off</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7902f142-e848-4f93-9016-82cac0a79bc5",
   "metadata": {},
   "source": [
    "Observed U-Shape of the test MSE curves : result of two competing properties of statistical learning methods\n",
    "- the expected test MSE, for a given value $x_0$, can always be decomposed into the sum of three fundamental quantities: \n",
    "    - the variance of $\\hat f(x_0)$ \n",
    "    - the squared bias of $\\hat f(x_0)$ \n",
    "    - the variance of the error terms $\\epsilon$.<br>\n",
    "<img src=\"img_19.JPG\" alt=\"image\" width=\"450\" height=\"auto\">\n",
    "- $E \\left[ \\left( y_0 - \\hat{f}(x_0) \\right)^2 \\right]$ defnes the expected test MSE at $x_0$, expected and refers to the average test MSE that we would obtain if we repeatedly test MSE estimated f using a large number of training sets, and tested each at $x_0$\n",
    "- The overall expected test MSE can be computed by averaging  $E \\left[ \\left( y_0 - \\hat{f}(x_0) \\right)^2 \\right]$ over all possible values of x0 in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436c00d-0d89-40cc-8579-7fe47bf84754",
   "metadata": {},
   "source": [
    "####  in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5801994c-0a8c-4b56-b103-c9b4272a3ef9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43537e0-cdbb-4c7d-a67f-2e48827e7b35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:orange\"> Variance</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc959dd7-2656-467e-9a3f-bc4906b9f136",
   "metadata": {},
   "source": [
    "- amount by which $\\hat f$ would change if we estimated it using a diferent training data set.\n",
    "- Since the training data are used to fit the statistical learning method, <mark>diferent training data sets will result in a diferent $\\hat f$. </mark>\n",
    "- But ideally the estimate for $f$ should not vary too much between training sets. \n",
    "- if a <mark>method has high variance then small changes in the training data can result in large changes in $\\hat f$. </mark>\n",
    "- <mark>more fexible statistical methods have higher variance.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7719326-9b6b-466e-8556-9b70139db54d",
   "metadata": {},
   "source": [
    "<img src=\"img_23.JPG\" alt=\"image\" width=\"1000\" height=\"auto\"><br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ca2ef-fe7c-427c-a2c4-e7be498cd7bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:orange\"> Bias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6839b-c9a6-4c80-8775-ffe007af7cfc",
   "metadata": {},
   "source": [
    "- the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.\n",
    "- Considering example of Liner regression model :\n",
    "    - linear regression assumes that there is a linear relationship between $Y$ and $X_1$, $X_2$,...,$X_p$.\n",
    "    - It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of $f$.\n",
    "- <mark>More flexible methods results  in less bias</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65dde1-f049-4335-a534-3cedb3ed3b7d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb545165-810b-4062-a151-203a51878b37",
   "metadata": {},
   "source": [
    "- as we use more <mark>fexible methods, the variance will increase and the bias will decrease. </mark>\n",
    "- <mark>The relative rate of change of these two quantities determines whether the test MSE increases or decreases.</mark>\n",
    "- As we increase the fexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. \n",
    "- However, at some point increasing fexibility has little impact on the bias but starts to signifcantly increase the variance. When this happens the test MSE increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51506a33-d019-4737-a3f6-8ec9961ae71e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f14642-1b62-4b52-ad57-a388d682b110",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> 2.2.3 The Classifcation Setting</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b157b47-4f4d-4a98-9800-b0d98f2d92ab",
   "metadata": {},
   "source": [
    "#### <u> Some modifications in classification setting</u>\n",
    "- Training observations $\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$, where now $y_1, \\ldots, y_n$ are qualitative.\n",
    "- Quantification of accuracy : \n",
    "    - the $training$ $error$ $rate$ : because it is computed based on the data  that was used to train our classifier.\n",
    "        - the proportion of mistakes that are made if we apply our estimate  $\\hat f$ to the training observations:\n",
    "        - ### $$\\frac{1}{n} \\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)$$\n",
    "            - $\\hat y_i$ : The predicted class label for the $i$-th observation using $\\hat{f}$.\n",
    "            - $I(y_i \\neq \\hat{y}_i)$ : \n",
    "                - $Indicator$ $Variable$ : an indicator variable that <mark>equals 1 if $y_i \\neq \\hat{y}_i$</mark> and <mark>0 if $y_i = \\hat{y}_i$.</mark> \n",
    "                - <mark>Indicator variable If $I(y_i \\neq \\hat{y}_i) = 0$ then the $i$th observation was classified correctly by our classification method; otherwise it was misclassified.</mark>\n",
    "    - the $testing$ $error$ $rate$ : associated with the test observations of the form : $(x_0, y_0)$\n",
    "        - ### $$\\text{Ave}(I(y_0 \\neq \\hat{y}_0))$$ \n",
    "            - $\\hat{y}_0$ :  is the predicted class label that results from applying the classifier to the test observation with predictor $x_0$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Good classifier : Test error rate is small\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7737f04a-ebbc-4cdd-8dd0-c998aaf43433",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:orange\"> The Bayes Classifer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899bef8-b125-4cc4-be5a-c14c44d27e79",
   "metadata": {},
   "source": [
    "- very simple classifer that assigns each observation to the most likely class, given its predictor values.\n",
    "- simply assign a test observation with predictor vector x0 to the class j for which \n",
    "#### $$ Pr(Y = j | X = x_0) $$  \n",
    " is largest.\n",
    "- Above is conditional probablity : It is the probability that $Y$ = $j$ , given that $X$ = $x_0$\n",
    "- In a <mark>two-class problem</mark> where there are only two possible response values, say class 1 or class 2, the <mark>Bayes classifier corresponds to predicting class one if $Pr(Y = 1|X = x_0) > 0.5$, and class two otherwise.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43fd2f3-e940-4cd1-89ea-47419a11e451",
   "metadata": {},
   "source": [
    "<img src=\"img_24.JPG\" alt=\"image\" width=\"900\" height=\"auto\"><br>\n",
    "<i>A simulated data set consisting of 100 observations in each of\n",
    "two groups, indicated in blue and in orange. The purple dashed line represents\n",
    "the Bayes decision boundary. The orange background grid indicates the region\n",
    "in which a test observation will be assigned to the orange class, and the blue\n",
    "background grid indicates the region in which a test observation will be assigned\n",
    "to the blue class.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1c01e-e776-4424-bfd9-a7fb374ac835",
   "metadata": {
    "tags": []
   },
   "source": [
    "- The Bayes classifier produces the lowest possible test error rate, called the <mark>Bayes error rate.</mark>\n",
    "- Since the Bayes classifier will always choose the class Bayes error rate for which $Pr(Y = j|X = x_0)$ is largest, the error rate will be $1 - \\max_j Pr(Y = j|X = x_0)$ at $X = x_0$. \n",
    "- In general, the overall Bayes error rate is given by \n",
    "### $$1 - E[\\max_j Pr(Y = j|X)]$$\n",
    " ,where the expectation averages the probability over all possible values of $X$. \n",
    "- For our simulated data, the Bayes error rate is 0.133. It is greater than zero because the classes overlap in the true population, which implies that $\\max_j Pr(Y = j|X = x_0) < 1$ for some values of $x_0$. The Bayes error rate is analogous to the irreducible error discussed earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8310b11-4c34-4c6f-9472-8c488db5151f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:orange\"> K-Nearest Neighbors</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08daa6a-3518-48b3-8307-53b1e57de404",
   "metadata": {},
   "source": [
    "- for real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifer is impossible.\n",
    "- Given positive integer $K$ and a test observation $x_0$, the KNN classifier first <mark>identifies the $K$ points in the training data that are closest to $x_0$, represented by $N_0$. </mark>\n",
    "- It then estimates the <mark>conditional probability for class $j$ as the fraction of points in $N_0$ whose response values equal $j$:\n",
    "$$Pr(Y = j|X = x_0) = \\frac{1}{K} \\sum_{i \\in N_0} I(y_i = j)$$</mark>\n",
    "- KNN classifes the test observation x0 to the class with the largest probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ebbda-7ff5-4c89-95c9-041e3c5dd428",
   "metadata": {},
   "source": [
    "<img src=\"img_25.JPG\" alt=\"image\" width=\"600\" height=\"auto\"><br>\n",
    "<i>The KNN approach, using K = 3, is illustrated in a simple\n",
    "situation with six blue observations and six orange observations. Left: a test\n",
    "observation at which a predicted class label is desired is shown as a black cross.\n",
    "The three closest points to the test observation are identifed, and it is predicted\n",
    "that the test observation belongs to the most commonly-occurring class, in this\n",
    "case blue. Right: The KNN decision boundary for this example is shown in black.\n",
    "The blue grid indicates the region in which a test observation will be assigned to\n",
    "the blue class, and the orange grid indicates the region in which it will be assigned\n",
    "to the orange class.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8e659-1e26-48ea-98fb-54d907f2e11e",
   "metadata": {},
   "source": [
    "- choosing the correct level of fexibility is critical to the success of any statistical learning method. The bias-variance tradeof, and the resulting U-shape in the test error, can make this a difcult task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff2af6-c3be-48e5-bff3-eeb64f0dab8c",
   "metadata": {},
   "source": [
    "<img src=\"img_26.JPG\" alt=\"image\" width=\"600\" height=\"auto\"><br>\n",
    "<i>With\n",
    "K = 1, the decision boundary is overly fexible, while with K = 100 it is not\n",
    "sufciently fexible. The Bayes decision boundary is shown as a purple dashed\n",
    "line. and black is for KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087deafc-5657-4b71-9ec9-41e507691809",
   "metadata": {},
   "source": [
    "<img src=\"img_27.JPG\" alt=\"image\" width=\"600\" height=\"auto\"><br>\n",
    "<i>The KNN training error rate (blue, 200 observations) and test\n",
    "error rate (orange, 5,000 observations) on the data from Figure 2.13, as the level\n",
    "of fexibility (assessed using 1/K on the log scale) increases, or equivalently as\n",
    "the number of neighbors K decreases. The black dashed line indicates the Bayes\n",
    "error rate. The jumpiness of the curves is due to the small size of the training\n",
    "data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cd7f7b-3ec7-4a19-9b30-9b243a60d1a0",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> 2.3 Lab: Introduction to Python</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6cf03-6735-4b90-b70b-ba99b68308b7",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> 2.3.1 Getting Started</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d1ca1-8a52-486a-a176-b1856b0393fa",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> 2.3.2 Basic Commands</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8605319f-12ba-4574-b6ab-352f2c08ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit a model with 11 variables\n"
     ]
    }
   ],
   "source": [
    "print('fit a model with', 11, 'variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cd4d6e-a003-488a-8ecb-c38541f80e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n",
       "\n",
       "Prints the values to a stream, or to sys.stdout by default.\n",
       "Optional keyword arguments:\n",
       "file:  a file-like object (stream); defaults to the current sys.stdout.\n",
       "sep:   string inserted between values, default a space.\n",
       "end:   string appended after the last value, default a newline.\n",
       "flush: whether to forcibly flush the stream.\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62c8c1b-7d22-4ab5-8b7b-cf13a15fdc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [3, 4, 5]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb55844-bb22-4378-b955-444903a8745a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 4, 9, 7]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [4, 9, 7]\n",
    "x+y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9931b-b8ac-460f-99e5-6190b929d51c",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> 2.3.3 Introduction to Numerical Python</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d244de7c-b43f-4c90-b6bd-3f900efb5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eec910d8-7f4f-4f76-80e8-6539d4f21b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([3, 4, 5])\n",
    "y = np.array([4, 9, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df89d4e-bf7c-49c1-9797-93911fc41c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 13, 12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb97ac67-31c9-4ca4-bef1-3e47a4a9b867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560f0272-6ada-47ae-a2ec-d2ebf1d3166c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86f5b1e7-d1f3-443a-87b2-f90da43fb48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e14064-de8d-4a2d-b447-37edb7068990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2], [3.0, 4]]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37562aaf-c868-4dd5-b5be-200056208867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n",
       "      like=None)\n",
       "\n",
       "Create an array.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "object : array_like\n",
       "    An array, any object exposing the array interface, an object whose\n",
       "    __array__ method returns an array, or any (nested) sequence.\n",
       "    If object is a scalar, a 0-dimensional array containing object is\n",
       "    returned.\n",
       "dtype : data-type, optional\n",
       "    The desired data-type for the array.  If not given, then the type will\n",
       "    be determined as the minimum type required to hold the objects in the\n",
       "    sequence.\n",
       "copy : bool, optional\n",
       "    If true (default), then the object is copied.  Otherwise, a copy will\n",
       "    only be made if __array__ returns a copy, if obj is a nested sequence,\n",
       "    or if a copy is needed to satisfy any of the other requirements\n",
       "    (`dtype`, `order`, etc.).\n",
       "order : {'K', 'A', 'C', 'F'}, optional\n",
       "    Specify the memory layout of the array. If object is not an array, the\n",
       "    newly created array will be in C order (row major) unless 'F' is\n",
       "    specified, in which case it will be in Fortran order (column major).\n",
       "    If object is an array the following holds.\n",
       "\n",
       "    ===== ========= ===================================================\n",
       "    order  no copy                     copy=True\n",
       "    ===== ========= ===================================================\n",
       "    'K'   unchanged F & C order preserved, otherwise most similar order\n",
       "    'A'   unchanged F order if input is F and not C, otherwise C order\n",
       "    'C'   C order   C order\n",
       "    'F'   F order   F order\n",
       "    ===== ========= ===================================================\n",
       "\n",
       "    When ``copy=False`` and a copy is made for other reasons, the result is\n",
       "    the same as if ``copy=True``, with some exceptions for 'A', see the\n",
       "    Notes section. The default order is 'K'.\n",
       "subok : bool, optional\n",
       "    If True, then sub-classes will be passed-through, otherwise\n",
       "    the returned array will be forced to be a base-class array (default).\n",
       "ndmin : int, optional\n",
       "    Specifies the minimum number of dimensions that the resulting\n",
       "    array should have.  Ones will be prepended to the shape as\n",
       "    needed to meet this requirement.\n",
       "like : array_like, optional\n",
       "    Reference object to allow the creation of arrays which are not\n",
       "    NumPy arrays. If an array-like passed in as ``like`` supports\n",
       "    the ``__array_function__`` protocol, the result will be defined\n",
       "    by it. In this case, it ensures the creation of an array object\n",
       "    compatible with that passed in via this argument.\n",
       "\n",
       "    .. versionadded:: 1.20.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "out : ndarray\n",
       "    An array object satisfying the specified requirements.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "empty_like : Return an empty array with shape and type of input.\n",
       "ones_like : Return an array of ones with shape and type of input.\n",
       "zeros_like : Return an array of zeros with shape and type of input.\n",
       "full_like : Return a new array with shape of input filled with value.\n",
       "empty : Return a new uninitialized array.\n",
       "ones : Return a new array setting values to one.\n",
       "zeros : Return a new array setting values to zero.\n",
       "full : Return a new array of given shape filled with value.\n",
       "\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When order is 'A' and `object` is an array in neither 'C' nor 'F' order,\n",
       "and a copy is forced by a change in dtype, then the order of the result is\n",
       "not necessarily 'C' as expected. This is likely a bug.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> np.array([1, 2, 3])\n",
       "array([1, 2, 3])\n",
       "\n",
       "Upcasting:\n",
       "\n",
       ">>> np.array([1, 2, 3.0])\n",
       "array([ 1.,  2.,  3.])\n",
       "\n",
       "More than one dimension:\n",
       "\n",
       ">>> np.array([[1, 2], [3, 4]])\n",
       "array([[1, 2],\n",
       "       [3, 4]])\n",
       "\n",
       "Minimum dimensions 2:\n",
       "\n",
       ">>> np.array([1, 2, 3], ndmin=2)\n",
       "array([[1, 2, 3]])\n",
       "\n",
       "Type provided:\n",
       "\n",
       ">>> np.array([1, 2, 3], dtype=complex)\n",
       "array([ 1.+0.j,  2.+0.j,  3.+0.j])\n",
       "\n",
       "Data-type consisting of more than one element:\n",
       "\n",
       ">>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n",
       ">>> x['a']\n",
       "array([1, 3])\n",
       "\n",
       "Creating an array from sub-classes:\n",
       "\n",
       ">>> np.array(np.mat('1 2; 3 4'))\n",
       "array([[1, 2],\n",
       "       [3, 4]])\n",
       "\n",
       ">>> np.array(np.mat('1 2; 3 4'), subok=True)\n",
       "matrix([[1, 2],\n",
       "        [3, 4]])\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " np.array?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ad45c-439a-42f4-9c29-6027baa87646",
   "metadata": {},
   "source": [
    "#### We could create a foating point array by passing a dtype argument into `np.array()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "013cc4b0-e3fd-4fd8-b1a5-1dfb56cbccee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.array([[1, 2], [3, 4]], float).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2b333-a81c-4e8f-ab7c-29efb9b3b6c1",
   "metadata": {},
   "source": [
    "#### The array x is two-dimensional. We can fnd out the number of rows and columns by looking at its `shape` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bd36490-acd7-4039-93dd-e725149072b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4412048-c167-4542-ab9d-5dfa2910cd01",
   "metadata": {},
   "source": [
    "A method is a function that is associated with an object. For instance, method given an array `x`, the expression `x.sum()` sums all of its elements, using the `sum()` method for arrays. The call `x.sum()` automatically provides `x` as the `.sum()` frst argument to its `sum()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d15da4f5-7d62-467f-af95-7c875715386d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4]).sum()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fce4d886-4cf2-46c1-b2c8-286f954f45f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Return the sum of a 'start' value (default: 0) plus an iterable of numbers\n",
       "\n",
       "When the iterable is empty, return the start value.\n",
       "This function is intended specifically for use with numeric values and may\n",
       "reject non-numeric types.\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c4edf05-7917-498f-a9e8-64a62a7d1ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "np.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74816e39-9dab-49b1-9794-2adf7c822338",
   "metadata": {},
   "source": [
    "As another example, the `reshape()` method returns a new array with the same elements as `x`, but a diferent shape. We do this by passing in a `tuple`in our call to `reshape()`, in this case `(2, 3)`. This `tuple` specifes that we would like to create a two-dimensional array with 2 rows and 3 columns.2 In what follows, the `\\n` character creates a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93bf74de-c442-4ed5-98db-ce3b8fe8e739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning x:\n",
      " [1 2 3 4 5 6]\n",
      "reshaped x:\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4, 5, 6])\n",
    "print('beginning x:\\n', x)\n",
    "x_reshape = x.reshape((2, 3))\n",
    "print('reshaped x:\\n', x_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de8c9fee-9b99-4ac2-8ed0-f6ac48214daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshape[1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6f42577-0273-472e-9980-94b6182f2681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshape[0, 0] = 5\n",
    "x_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf83f627-e55e-45aa-8b11-f13422d62b6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m my_tuple \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m my_tuple[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "my_tuple = (3, 4, 5)\n",
    "my_tuple[0] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29f4230a-3dc4-4c40-8fb4-df3d1c7ac53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e2676a3-cfec-4c24-a9f9-28c8a620afea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshape.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b10ba39b-39a4-4de2-aac3-2fd424670727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x_reshape.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "834e46b2-8462-4147-b5dd-758972c6471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshape.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01c746fe-72c4-4403-ba18-ba5d0746efd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.23606798, 1.41421356, 1.73205081, 2.        , 2.23606798,\n",
       "       2.44948974])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.sqrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa752ba3-29b5-4460-9baa-20e34c450d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25,  4,  9, 16, 25, 36])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44668524-5963-48be-baeb-fcb3cd60a693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "474f48f1-c23c-46c0-8554-292a756d6e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.58337834,  0.15025811, -1.25001053,  0.50727696,  0.77981077,\n",
       "        0.43181605,  0.34985758, -1.10198146, -0.07578567,  1.01083682,\n",
       "        0.68567432, -1.22068069,  1.391352  , -0.13935186, -0.90872787,\n",
       "        0.16922581,  0.10220595,  0.46602399,  0.63387923, -0.54590838,\n",
       "       -1.55782983,  1.09908744, -1.10637733, -0.03283059, -0.94137151,\n",
       "       -0.62905054, -0.1861669 , -0.91790425, -0.33250818, -2.2507106 ,\n",
       "        0.14063771,  0.72613496, -0.77030007,  0.8188484 ,  0.70971799,\n",
       "        0.04841052,  0.44419468, -1.43543034, -0.36806104,  1.18264541,\n",
       "       -1.91208697, -0.15231121, -0.67119985, -0.08796113,  0.28885291,\n",
       "       -0.4762306 , -0.47246805, -1.25829248,  0.02260674,  0.31407923])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.normal(size=50)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58f74c7c-fda8-416c-bcea-911be162146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + np.random.normal(loc=50, scale=1, size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832bfa4-949c-4014-a9fc-b49d0139763a",
   "metadata": {},
   "source": [
    "The np.corrcoef() function computes the correlation matrix between x and np.corrcoef()\n",
    "y. The of-diagonal elements give the correlation between x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb2f069b-2155-41c5-b087-82ff569c269c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.65017786],\n",
       "       [0.65017786, 1.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4280dd77-e528-488a-b15d-e7528f6ec4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.48356396  0.76610779 -0.63922132  1.35222002]\n",
      "[2.27047633 2.12633888]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.normal(scale=1, size=4))\n",
    "print(np.random.normal(scale=1, size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab61e303-b7f7-48fc-9678-a903668d0b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.78774824  1.44877901]\n",
      "[-5.78774824  1.44877901]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(100)\n",
    "print(rng.normal(scale=5, size=2))\n",
    "rng2 = np.random.default_rng(100)\n",
    "print(rng2.normal(scale=5, size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6732d590-ba70-4a07-a19b-15d7d2b5af3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac0f02-8acd-4916-b07e-a7089c6cdedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
